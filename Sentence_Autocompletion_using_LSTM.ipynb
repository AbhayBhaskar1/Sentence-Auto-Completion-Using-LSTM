{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('Shakespeare.csv')\n",
        "print(data.head())\n",
        "\n",
        "# Getting text from the data\n",
        "text = [i for i in data['PlayerLine']]\n",
        "\n",
        "# Text Cleaning\n",
        "def clean_text(text):\n",
        "    # Removing special characters and digits\n",
        "    text = re.sub('[^a-zA-Z\\s]', '', text)\n",
        "    text = re.sub('\\d+', '', text)\n",
        "    # Converting text to lower case\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "texts = [clean_text(t) for t in text]\n",
        "\n",
        "# Let's take the first 10000 words for the model training\n",
        "texts = texts[:10000]\n",
        "\n",
        "# Using tensorflow tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Generating text sequences\n",
        "text_sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Padding the sequences\n",
        "max_sequence_len = max([len(x) for x in text_sequences])\n",
        "text_sequences = pad_sequences(text_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "print('Maximum Sequence Length -->>', max_sequence_len)\n",
        "print('Text Sequence -->>\\n', text_sequences[0])\n",
        "print('Text Sequence Shape -->>', text_sequences.shape)\n",
        "\n",
        "# Getting X and y from the data\n",
        "X, y = text_sequences[:, :-1], text_sequences[:, -1]\n",
        "print('First Input :', X[0])\n",
        "print('First Target :', y[0])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Using one-hot encoding on y\n",
        "total_words = len(word_index) + 1\n",
        "print('Total Number of Words:', total_words)\n",
        "\n",
        "y = to_categorical(y, num_classes=total_words)\n",
        "\n",
        "# Printing X and y shapes\n",
        "print('Input Shape :', X.shape)\n",
        "print('Target Shape :', y.shape)\n",
        "\n",
        "# Building the model\n",
        "model = Sequential(name=\"LSTM_Model\")\n",
        "model.add(Embedding(total_words, max_sequence_len-1, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(512, return_sequences=False))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "# Printing model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Training the LSTM model\n",
        "history = model.fit(X, y, epochs=50, verbose=1)\n",
        "\n",
        "# Function for text autocompletion\n",
        "def autoCompletations(text, model):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    sequence = pad_sequences(sequence, maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted_word_index = np.argmax(model.predict(sequence, verbose=0))\n",
        "    predicted_word = ''\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted_word_index:\n",
        "            predicted_word = word\n",
        "            break\n",
        "    return text + ' ' + predicted_word\n",
        "\n",
        "# Generate text with specified number of new words\n",
        "def generate_text(text, new_words):\n",
        "    for _ in range(new_words):\n",
        "        text = autoCompletations(text, model)\n",
        "    return text\n",
        "\n",
        "# Example of generated text\n",
        "generated_text = generate_text('I have seen', 5)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNasJ8SL0GZY",
        "outputId": "15bff723-15bb-4ec4-e07f-680d72a86ece"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          PlayerLine\n",
            "0  *Project Gutenberg's Etext of Tom Swift And Hi...\n",
            "1                                                 \\n\n",
            "2     #4 in the Victor Appleton's Tom Swift Series\\n\n",
            "3                                                 \\n\n",
            "4  We name these Etext files as they are numbered...\n",
            "Maximum Sequence Length: 16\n",
            "Text Sequence:\n",
            " [   0    0    0    0    0    0  183 1415  163    4   14   34    3   18\n",
            "   33  116]\n",
            "Text Sequence Shape: (10000, 16)\n",
            "First Input: [   0    0    0    0    0    0  183 1415  163    4   14   34    3   18\n",
            "   33]\n",
            "First Target: 116\n",
            "Total Number of Words: 3813\n",
            "Input Shape: (10000, 15)\n",
            "Target Shape: (10000, 3813)\n",
            "Model: \"LSTM_Model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 15, 15)            57195     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 512)               1081344   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3813)              1956069   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3094608 (11.80 MB)\n",
            "Trainable params: 3094608 (11.80 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "313/313 [==============================] - 17s 45ms/step - loss: 3.2246 - accuracy: 0.6172\n",
            "Epoch 2/50\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 2.5426 - accuracy: 0.6425\n",
            "Epoch 3/50\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 2.4418 - accuracy: 0.6424\n",
            "Epoch 4/50\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 2.3744 - accuracy: 0.6425\n",
            "Epoch 5/50\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 2.3105 - accuracy: 0.6429\n",
            "Epoch 6/50\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 2.2467 - accuracy: 0.6469\n",
            "Epoch 7/50\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 2.1796 - accuracy: 0.6470\n",
            "Epoch 8/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 2.1052 - accuracy: 0.6493\n",
            "Epoch 9/50\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 2.0331 - accuracy: 0.6485\n",
            "Epoch 10/50\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.9506 - accuracy: 0.6529\n",
            "Epoch 11/50\n",
            "313/313 [==============================] - 2s 8ms/step - loss: 1.8604 - accuracy: 0.6567\n",
            "Epoch 12/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.7621 - accuracy: 0.6610\n",
            "Epoch 13/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.6543 - accuracy: 0.6717\n",
            "Epoch 14/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.5343 - accuracy: 0.6809\n",
            "Epoch 15/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.4170 - accuracy: 0.6987\n",
            "Epoch 16/50\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2934 - accuracy: 0.7146\n",
            "Epoch 17/50\n",
            "313/313 [==============================] - 2s 8ms/step - loss: 1.1688 - accuracy: 0.7407\n",
            "Epoch 18/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.0516 - accuracy: 0.7649\n",
            "Epoch 19/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.9365 - accuracy: 0.7941\n",
            "Epoch 20/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.8453 - accuracy: 0.8157\n",
            "Epoch 21/50\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.7493 - accuracy: 0.8376\n",
            "Epoch 22/50\n",
            "313/313 [==============================] - 2s 8ms/step - loss: 0.6749 - accuracy: 0.8591\n",
            "Epoch 23/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.6106 - accuracy: 0.8727\n",
            "Epoch 24/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.5541 - accuracy: 0.8892\n",
            "Epoch 25/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.4938 - accuracy: 0.9048\n",
            "Epoch 26/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.4538 - accuracy: 0.9170\n",
            "Epoch 27/50\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.4341 - accuracy: 0.9215\n",
            "Epoch 28/50\n",
            "313/313 [==============================] - 2s 8ms/step - loss: 0.4040 - accuracy: 0.9257\n",
            "Epoch 29/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.3824 - accuracy: 0.9313\n",
            "Epoch 30/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.3685 - accuracy: 0.9362\n",
            "Epoch 31/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.3478 - accuracy: 0.9403\n",
            "Epoch 32/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.3269 - accuracy: 0.9460\n",
            "Epoch 33/50\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.3225 - accuracy: 0.9458\n",
            "Epoch 34/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.3084 - accuracy: 0.9490\n",
            "Epoch 35/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.3123 - accuracy: 0.9475\n",
            "Epoch 36/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.3005 - accuracy: 0.9523\n",
            "Epoch 37/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.2903 - accuracy: 0.9541\n",
            "Epoch 38/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.2990 - accuracy: 0.9509\n",
            "Epoch 39/50\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.2912 - accuracy: 0.9525\n",
            "Epoch 40/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.2774 - accuracy: 0.9555\n",
            "Epoch 41/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.2760 - accuracy: 0.9565\n",
            "Epoch 42/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.2717 - accuracy: 0.9563\n",
            "Epoch 43/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.2632 - accuracy: 0.9594\n",
            "Epoch 44/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.2519 - accuracy: 0.9605\n",
            "Epoch 45/50\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.2509 - accuracy: 0.9618\n",
            "Epoch 46/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.2532 - accuracy: 0.9598\n",
            "Epoch 47/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.2571 - accuracy: 0.9591\n",
            "Epoch 48/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.2561 - accuracy: 0.9590\n",
            "Epoch 49/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.2490 - accuracy: 0.9597\n",
            "Epoch 50/50\n",
            "313/313 [==============================] - 2s 8ms/step - loss: 0.2556 - accuracy: 0.9597\n",
            "I love house it water earnestly suppose\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generate_text('my soul ',2)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SIYStlU1zJf",
        "outputId": "7b6de2cb-65f7-40c4-c171-a467460f041e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my soul  residence far\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model\n",
        "model.save('sentence_completion.h5')\n",
        "\n",
        "# saving the tokenizer\n",
        "filename = 'tokenizer.pkl'\n",
        "pickle.dump(tokenizer, open(filename, 'wb'))\n"
      ],
      "metadata": {
        "id": "qM9SZy6_yxAX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pZD51W887NMA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}